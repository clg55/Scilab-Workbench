\chapter[Representation of Signals]{Time and Frequency Representation of Signals}

\section{Frequency Response}
\subsection{Bode Plots}
\index{Bode plot}
\label{bode}

	The Bode plot is used to plot the phase
and log-magnitude response of functions of a single
complex variable.  Historically, the Bode plot was
used to plot rational polynomial
transfer functions.  The log-scale characteristics
of the Bode plot permitted a rapid, ``back-of-the-envelope''
calculation of a system's magnitude and
phase response.  In the  following discussion of Bode plots
we consider only real, causal systems.
Consequently, any poles and zeros of the system occur
in complex conjugate pairs (or are strictly real) and
the poles are all located in the left-half $s$-plane.

	A Bode plot consists of two separate plots.  One
plot is the magnitude response and the other
is the phase response.  For $H(s)$ a transfer function 
of the complex variable $s$, the 
log-magnitude of $H(s)$ is defined by
%
\begin{equation}
M(\omega)=20\log_{10}|H(s)_{s=j\omega}|
\label{e.bode.1}
\end{equation}
%
and the phase of $H(s)$ is defined by
%
\begin{equation}
\Theta(\omega)=\tan^{-1}[\frac{Im(H(s)_{s=j\omega})}{Re(H(s)_{s=j\omega})}]
\label{e.bode.2}
\end{equation}
%
where $j=\sqrt{-1}$ and $\omega$ is a real variable.  The
magnitude, $M(\omega)$, is plotted on a log-linear scale
where the independent axis is marked in decades (sometimes
in octaves) of degrees or radians and the dependent axis is marked
in decibels.  The phase, $\Theta(\omega)$, is also plotted on a log-linear
scale where, again, the independent axis is marked as is the magnitude plot
and the dependent axis is marked in degrees (and sometimes radians).

	When $H(s)$ is a rational polynomial
it can be expressed as
%
\begin{equation}
H(s)=C\frac{\prod_{n=1}^{N}(s-a_n)}{\prod_{m=1}^{M}(s-b_m)}
\label{e.bode.3}
\end{equation}
%
where the $a_n$ and $b_m$ are real or complex constants
representing the zeros and poles, respectively, of $H(s)$,
and $C$ is a real scale factor.
For the moment let us assume that the $a_n$ and $b_m$ are
strictly real.  Evaluating (\ref{e.bode.3}) on the $j\omega$-axis
we obtain
%
\begin{eqnarray}
H(j\omega)&=&C\frac{\prod_{n=1}^{N}(j\omega-a_n)}{\prod_{m=1}^{M}(j\omega-b_m)}\nonumber\\
&=&C\frac{\prod_{n=1}^{N}\sqrt{\omega^2+a_n^2}e^{j\tan^{-1}\omega/(-a_n)}}{\prod_{m=1}^{M}\sqrt{\omega^2+b_m^2}e^{j\tan^{-1}\omega/(-b_m)}}
\label{e.bode.4}
\end{eqnarray}
%
where each term in the above products has been converted
from rectangular form to polar form.  Using the $H(j\omega)$ obtained
in (\ref{e.bode.4}) we
calculated the log-magnitude and phase response by using (\ref{e.bode.1})
and (\ref{e.bode.2}) to obtain
%
\begin{equation}
M(\omega)=\sum_{n=1}^{N}20\log\sqrt{\omega^2+a_n^2}-\sum_{m=1}^M\sqrt{\omega^2+b_m^2}
\label{e.bode.5}
\end{equation}
%
and
%
\begin{equation}
\Theta(\omega)=\sum_{n=1}^{N}\tan^{-1}(\omega/(-a_n))-\sum_{m=1}^M\tan^{-1}(\omega/(-b_m)).
\label{e.bode.6}
\end{equation}
%

	To see how the Bode plot is
constructed assume that both (\ref{e.bode.5}) and (\ref{e.bode.6}) consist
of single terms corresponding to a pole of $H(s)$.  Consequently, the 
magnitude and phase become
%
\begin{equation}
M(\omega)=-20\log\sqrt{\omega^2+a^2}
\label{e.bode.7}
\end{equation}
%
and
%
\begin{equation}
\Theta(\omega)=-j\tan^{-1}(\omega/(-a)).
\label{e.bode.8}
\end{equation}
%
We plot the magnitude in (\ref{e.bode.7}) using two straight line
approximations.  That is, for $|\omega|\ll|a|$ we have that $M(\omega)\approx
-20\log|a|$ which is a constant (i.e., a straight line with zero slope).
For $|\omega|\gg|a|$ we have that $M(\omega)\approx-20\log|\omega|$ which
is a straight line on a log scale which has a slope of -20 db/decade.
The intersection of these two straight lines is  at $w=a$.  
Figure~\ref{f.bode.1} illustrates these two straight line approximations
for $a=10$.
%
\input{\Figdir bode.1.tex}
\dessin{{\tt exec('bode.1.code')} Log-Magnitude Plot of $H(s)=1/(s-a)$}{f.bode.1}
%
When $\omega=a$ we have that $M(\omega)=-20\log\sqrt{2}a=-20\log a
-20\log\sqrt{2}$.  Since $20\log\sqrt{2}=3.0$ we have that at $\omega=a$
the correction to the straight line approximation is $-3$db.
Figure~\ref{f.bode.1} illustrates the true magnitude response of 
$H(s)=(s-a)^{-1}$ for $a=10$
and it can be seen that the straight line approximations
with the 3db correction at $\omega=a$ yields very satisfactory results.
The phase in (\ref{e.bode.8}) can also be approximated.  For $\omega\ll a$
we have $\Theta(\omega)\approx 0$ and for $\omega\gg a$ we have
$\Theta(\omega)\approx -90^{\circ}$.  At $\omega=a$ we have $\Theta(\omega)
=-45^{\circ}$.  Figure~\ref{f.bode.2} illustrates the straight line 
approximation to $\Theta(\omega)$ as well as the actual phase response.
%
\input{\Figdir bode.2.tex}
\dessin{{\tt exec('bode.2.code')} Phase Plot of $H(s)=1/(s-a)$}{f.bode.2}
%

	For transfer functions which have more than one term
in the expressions (\ref{e.bode.5}) and (\ref{e.bode.6}) the
straight line approximation procedure is repeated for each
term and then the superposition
of these terms yields the Bode plot.  In the case
where the  poles and zeros of $H(s)$ are not all real
but occur in conjugate pairs (which is always the case
for real systems) one other type of term must be considered.
This term is of the form
%
\begin{eqnarray}
H(s)&=&\frac{1}{[s-(a+jb)][s-(a-jb)]}\nonumber\\
    &=&\frac{1}{s^2-2as+(a^2+b^2)}
\label{e.bode.9}
\end{eqnarray}
%
where $a$ and $b$ are real.  Evaluating (\ref{e.bode.9}) for $s=j\omega$
yields
%
\begin{eqnarray}
H(s)&=&\frac{1}{(a^2+b^2-\omega^2)-2aj\omega}\nonumber\\
    &=&\frac{1}{\sqrt{\omega^4+2(a^2-b^2)\omega^2+(a^2+b^2)}\exp(j\tan^{-1}[\frac{-2a\omega}{a^2+b^2-\omega^2}])}.
\label{e.bode.10}
\end{eqnarray}
%
For $\omega$ very small, the magnitude component in (\ref{e.bode.10}) is
approximately $1/(a^2+b^2)$ and for $\omega$ very large the magnitude
becomes approximately $1/\omega^2$.  Consequently, for small $\omega$
the magnitude response can be approximated by the straight line 
$M(\omega)\approx-20\log_{10}|a^2+b^2|$ and for $\omega$ large we have
$M(\omega)\approx-20\log|\omega^2|$ which is a 
straight line with a slope of -40db/decade.  These two straight lines
intersect at $\omega=\sqrt{a^2+b^2}$.  Figure~\ref{f.bode.3} illustrates
%
\input{\Figdir bode.3.tex}
\dessin{{\tt exec('bode.3.code')} Log-Magnitude Plot of $H(s)=(s^2-2as+(a^2+b^2))^{-1}$}
{f.bode.3}

%
the straight line approximations for $a=10$ and $b=25$.  The behavior of
the magnitude plot when $\omega$ is neither small nor large with
respect to $a$ and $b$ depends on whether $b$ is greater than
$a$ or not.  In the case where $b$ is less than $a$, the
magnitude plot is similar to the case where the roots of the transfer
function are strictly real, and consequently, the magnitude varies 
monotonically between the two straight line approximations
shown in Figure~\ref{f.bode.3}.  The correction at $\omega=\sqrt{a^2+b^2}$
is -6db plus $-20\log|a/(\sqrt{a^2+b^2})|$.  For $b$ greater than $a$,
however, the term in (\ref{e.bode.10}) exhibits resonance.  This 
resonance is manifested as a local maximum of the magnitude response
which occurs at $\omega=\sqrt{b^2-a^2}$.  The value of the magnitude
response at this maximum is $-20\log|2ab|$.  The effect of resonance
is illustrated in Figure~\ref{f.bode.3} as the upper dotted curve. 
Non-resonant behavior is illustrated in Figure~\ref{f.bode.3} by the
lower dotted curve.

	The phase curve for the expression in (\ref{e.bode.10})
is approximated as follows.  For $\omega$ very small the imaginary
component of (\ref{e.bode.10}) is small and the real part
is non-zero.  Thus, the phase is approximately zero.
For $\omega$ very large the real part of (\ref{e.bode.10}) dominates the
imaginary part and, consequently, the phase is approximately
$-180^{\circ}$.  At $\omega=\sqrt{a^2+b^2}$ the real part of (\ref{e.bode.10})
is zero and the imaginary part is negative so that the phase is exactly
$-90^{\circ}$.  The phase curve is shown in Figure~\ref{f.bode.4}.
%
\input{\Figdir bode.4.tex}
\dessin{{\tt exec('bode.4.code')} Phase Plot of $H(s)=(s^2-2as+(a^2+b^2))^{-1}$}
{f.bode.4}

%

\subsubsection{How to Use the Macro {\tt bode}}
\index{Bode plot,macro syntax}
\index{macro syntax!bode@{\tt bode}}

	The macro {\tt bode} calculates and displays the Bode plot
described in the previous section.  The macro calculates the
Bode plot exactly according to its definition in (\ref{e.bode.1}) and
(\ref{e.bode.2}).  The description of the transfer function can take
two forms.  In the first form the transfer function is given explicitly
as a rational polynomial.  In the second form the transfer function
is given implicitly as a state-space description of a linear system.  
In both cases
the user is obligated to declare the passed object (either rational polynomial
or state-space description) using the linear system
macro {\tt syslin}).

	When using an explicit representation of the transfer function
the syntax of the call to {\tt bode} is as follows
\begin{verbatim}
-->bode(h,fmin,fmax[,step][,comments])
\end{verbatim}
where {\tt h} is a rational polynomial whose fourth argument is
either {\tt 'c'} or {\tt 'd'} indicating that the rational polynomial
represents, respectively, a continuous-time transfer function or a
discrete-time transfer function (the default value for the 
fourth argument of a rational polynomial is {\tt []}).
The fourth argument of a rational polynomial 
can be set in two ways, either directly, as follows (for a continuous-time
system)
\begin{verbatim}
-->h(4)='c'
\end{verbatim}
or by using the macro {\tt syslin} as follows (for a discrete-time system)
\begin{verbatim}
-->h=syslin('c',h).
\end{verbatim}
The variables {\tt fmin} and {\tt fmax}
indicate the minimum and maximum values displayed by the Bode plot.
The variables {\tt step} and {\tt comments} are optional.  The variable
{\tt step} indicates the spacing between sampled values of the transfer
function in units of decades of Hertz.  The default value of {\tt pas}
is .1 and, consequently, the default sampling is 10 samples for every
decade in Hertz.  The variable {\tt comments} permits the user to
annotate plots obtained by using {\tt bode}.

	When using an implicit representation of the transfer function
the syntax of the call to {\tt bode} is as follows
\begin{verbatim}
-->bode(sl,fmin,fmax[,pas][,comments])
\end{verbatim}
where the only difference between this syntax and 
the first syntax is that {\tt sl}
is an implicit representation of the transfer function using a
a state-space system representation.  The implicit representation is
obtained as follows
\begin{verbatim}
-->sl=syslin(domain,a,b,c[,d][,x0])
\end{verbatim}
where {\tt a}, {\tt b}, {\tt c}, {\tt d}, and {\tt x0} are the parameters
of a linear system following the format for a continuous system
if {\tt domain='c'} and following the format for a discrete system
if {\tt domain='d'}.  The continuous time system assumes the following
form
%
\begin{eqnarray}
\dot{x}(t)&=&\mtt{a}x(t)+\mtt{b}u(t)\nonumber\\
y(t)&=&\mtt{c}x(t)+\mtt{d}w(t)\nonumber
\end{eqnarray}
%
and {\tt x0} is the initial condition.  The parameters {\tt d} and {\tt x0}
are optional and take the default value of zero if not provided by the user.
The discrete time system takes the form
%
\begin{eqnarray}
x(n+1)&=&\mtt{a}x(n)+\mtt{b}u(n)\nonumber\\
y(n)&=&\mtt{c}x(n)+\mtt{d}w(n)\nonumber
\end{eqnarray}
%
and {\tt x0} is the initial condition.  The parameters {\tt d} and {\tt x0}
are again optional and take the default value of zero if not provided 
by the user.

\subsubsection{Examples Using {\tt bode}}
\index{Bode plot!examples}

	Here are presented three examples.  The first example
illustrates how to use the macro {\tt bode} when the user has
an implicit state-space description of a system.
The second example shows how to use {\tt bode} when the transfer
function of interest is explicitly defined as a rational polynomial
in $s$.  The third example gives the Bode plot of the two previous
systems when these two systems are connected in series.

	In the first example, the system is defined by the
state-space description below
%
\begin{eqnarray}
\dot{x}&=&-2\pi x+u\nonumber\\
y&=&18\pi x+u.\nonumber
\end{eqnarray}
%
The initial condition is not important since the Bode plot is of
the steady state behavior of the system.  The $\Psi$Lab commands used
to create the above system as well as the call to the macro {\tt bode}
are listed below.
\verbatok{\Figdir bode.5.dia}
\end{verbatim}
The result of the call to {\tt bode} for this example is illustrated
in Figure~\ref{f.bode.5}.
%
\input{\Figdir bode.5.tex}
\dessin{{\tt exec('bode.5.code')} Bode Plot of State-Space System Representation}
{f.bode.5}

%

	The following example illustrates the use of the {\tt bode}
macro when the user has an explicit rational polynomial representation
of the system.  The following $\Psi$Lab commands were used to define
the system and to make the call to {\tt bode}.
\verbatok{\Figdir bode.6.dia}
\end{verbatim}
The result of the call to {\tt bode} for this example is illustrated
in Figure~\ref{f.bode.6}.
%
\input{\Figdir bode.6.tex}
\dessin{{\tt exec('bode.6.code')} Bode Plot of Rational Polynomial System Representation}
{f.bode.6}

%

	The final example combines the systems used in the two previous
examples by attaching them together in series.  The state-space description
is converted to a rational polynomial description using the {\tt ss2tf}
macro and then the two transfer functions are multiplied together before
being passed to {\tt bode}.  The $\Psi$Lab commands are as follows.
\verbatok{\Figdir bode.7.dia}
\end{verbatim}
Notice that the rational polynomial which results from the call to
the macro {\tt ss2tf} automatically has its fourth argument set to
the value {\tt 'c'}.
The result of the call to {\tt bode} for this example is illustrated
in Figure~\ref{f.bode.7}.
%
\input{\Figdir bode.7.tex}
\dessin{{\tt exec('bode.7.code')} Bode Plot Combined Systems}
{f.bode.7}

%


\subsubsection{Macro Code for {\tt bode}}
\index{macro code!bode@{\tt bode}}
\index{Bode plot!macro code}

	The macro {\tt bode} is part of the library of automatic
control macros found in the file {\tt auto.sci}and the text of the 
macro can be found there.
%\verbatok{../macros/bode.sci}
%\end{verbatim}

\subsection{Phase and Group Delay}
\label{group}
\index{group delay}
\index{phase delay}

	In the theory of narrow band filtering
there are two parameters which characterize the 
effect that band pass filters have on narrow band
signals.  These two parameters are called phase delay
and group delay.  Here, we define the phase and
group delay and we give an explanation of their
significance.

	Let $H(\omega)$ denote the Fourier transform of
a system
%
\begin{equation}
H(\omega)=A(\omega)e^{j\theta(\omega)}
\label{e0.1.1}
\end{equation}
%
where $A(\omega)$ is the magnitude of $H(\omega)$ and
$\theta(\omega)$ is the phase of $H(\omega)$.  Then the phase delay, 
$t_p(\omega)$, and the group delay, $t_g(\omega)$, are defined by
%
\begin{equation}
t_p(\omega)=\theta(\omega)/\omega
\label{e0.1.2}
\end{equation}
%
and
%
\begin{equation}
t_g(\omega)=d\theta(\omega)/d\omega.
\label{e0.1.3}
\end{equation}
%
Now assume that $H(\omega)$ represents an ideal
band pass filter.  By ideal we mean that
the magnitude of $H(\omega)$ is a non-zero constant for
$\omega_0-\omega_c<|\omega|<\omega_0+\omega_c$
and zero otherwise, and that the phase of 
$H(\omega)$ is linear plus a constant in these regions.
Furthermore, the impulse response of $H(\omega)$
is real.  Consequently, the magnitude of $H(\omega)$
has even symmetry and the phase of $H(\omega)$ has odd
symmetry.  


	Since the phase of $H(\omega)$ is linear plus
a constant it can be expressed as
%
\begin{equation}
\theta(\omega)=\left\{\begin{array}{ll}
 \theta(\omega_0)+\theta'(\omega_0)(\omega-\omega_0),&\mbox{$\omega>0$}\\
 -\theta(\omega_0)+\theta'(\omega_0)(\omega+\omega_0),&\mbox{$\omega<0$}
\end{array}\right.
\label{e0.1.4}
\end{equation}
%
where $\omega_0$ represents the center frequency of the band pass
filter.  The possible discontinuity of the phase at
$\omega=0$ is necessary due to the fact that $\theta(\omega)$ must be an
odd function.  The expression in (\ref{e0.1.4}) can be rewritten
using the definitions for phase and group delay in (\ref{e0.1.2})
and (\ref{e0.1.3}).  This yields
%
\begin{equation}
\theta(\omega)=\left\{\begin{array}{ll}
 \omega_0t_p+(\omega-\omega_0)t_g,&\mbox{$\omega>0$}\\
 -\omega_0t_p+(\omega+\omega_0)t_g,&\mbox{$\omega<0$}
\end{array}\right.
\label{e0.1.5}
\end{equation}
%
where, now, we take $t_p=t_p(\omega_0)$ and $t_g=t_g(\omega_0)$.

	Now assume that a signal, $f(t)$, is to 
be filtered by $H(\omega)$ where $f(t)$ is composed
of a modulated band-limited signal.  That is,
%
\begin{equation}
f(t)=f_l(t)\cos(\omega_0t)
\label{e0.1.6}
\end{equation}
%
where $\omega_0$ is the center frequency of the
band pass filter and $F_l(\omega)$ is the Fourier transform
a the bandlimited signal $f_l(t)$
($F_l(\omega)=0$ for $|\omega|>\omega_c$).  It is now
shown that the output of the filter due to the input in 
(\ref{e0.1.6}) takes the following form
%
\begin{equation}
g(t)=f_l(t+t_g)\cos[\omega_0(t+t_p)].
\label{e0.1.7}
\end{equation}
%

	To demonstrate the validity of (\ref{e0.1.7}) the Fourier transform
of the input in (\ref{e0.1.6}) is written as
%
\begin{equation}
F(\omega)=\frac{1}{2}[F_l(\omega-\omega_0)+F_l(\omega+\omega_0)]
\label{e0.1.8}
\end{equation}
%
where 
(\ref{e0.1.8}) represents the convolution of $F_l(\omega)$ with
the Fourier transform of $\cos(\omega_0t)$.
The Fourier transform of the filter, $H(\omega)$, can be written
%
\begin{equation}
H(\omega)=\left\{\begin{array}{ll}
 e^{\omega_0t_p+(\omega-\omega_0)t_g},
	&\mbox{$\omega_0-\omega_c<\omega<\omega_0+\omega_c$}\\
 e^{-\omega_0t_p+(\omega+\omega_0)t_g},
	&\mbox{$-\omega_0-\omega_c<\omega<-\omega_0+\omega_c$}\\
 0,
	&\mbox{otherwise}\\
\end{array}\right.
\label{e0.1.9}
\end{equation}
%
Thus, since $G(\omega)=F(\omega)H(\omega)$,
%
\begin{equation}
G(\omega)=\left\{\begin{array}{ll}
\frac{1}{2}F_l(\omega-\omega_0) e^{\omega_0t_p+(\omega-\omega_0)t_g},
	&\mbox{$\omega_0-\omega_c<\omega<\omega_0+\omega_c$}\\
\frac{1}{2}F_l(\omega+\omega_0) e^{-\omega_0t_p+(\omega+\omega_0)t_g},
	&\mbox{$-\omega_0-\omega_c<\omega<-\omega_0+\omega_c$}
\end{array}\right.
\label{e0.1.10}
\end{equation}
%
Calculating $g(t)$ using the inverse Fourier transform
%
\begin{eqnarray}
g(t)&=&\frac{1}{2\pi}\int_{-\infty}^{\infty}F(\omega)H(\omega)\nonumber\\
    &=&\frac{1}{2}\frac{1}{2\pi}[\int_{\omega_0-\omega_c}^{\omega_0+\omega_c}
       F_l(\omega-\omega_0)e^{j[(\omega-\omega_0)t_g+\omega_0t_p]}e^{j\omega t}d\omega\nonumber\\
    && +\int_{-\omega_0-\omega_c}^{-\omega_0+\omega_c}
F_l(\omega+\omega_0)e^{j[(\omega+\omega_0)t_g-\omega_0t_p]}e^{j\omega t}d\omega]
\label{e0.1.11}
\end{eqnarray}
%
Making the change in variables $u=\omega-\omega_0$ and $v=\omega+\omega_0$yields
%
\begin{eqnarray}
g(t)&=&\frac{1}{2}\frac{1}{2\pi}[\int_{-\omega_c}^{\omega_c}
       F_l(u)e^{j[ut_g+\omega_0t_p]}e^{jut}e^{j\omega_0t}du\nonumber\\
    && +\int_{-\omega_c}^{\omega_c}
F_l(v)e^{j[vt_g-\omega_0t_p]}e^{jvt}e^{-j\omega_0 t}dv]
\label{e0.1.12}
\end{eqnarray}
%
Combining the integrals and performing some algebra gives
%
\begin{eqnarray}
g(t)&=&\frac{1}{2}\frac{1}{2\pi}\int_{-\omega_c}^{\omega_c}
       F_l(\omega)e^{j\omega t_g}e^{j\omega t}[e^{j\omega_0t_p}e^{j\omega_0t}
      +e^{-j\omega_0t_p}e^{-j\omega_0t}]d\omega\nonumber\\
    &=&\frac{1}{2\pi}\int_{-\omega_c}^{\omega_c}
       F_l(\omega)\cos[\omega_0(t+t_p)]
      e^{j\omega(t+t_g)}d\omega\nonumber\\
    &=&\cos[\omega_0(t+t_p)]\frac{1}{2\pi}\int_{-\omega_c}^{\omega_c}
       F_l(\omega)e^{j\omega(t+t_g)}d\omega\nonumber\\
    &=&\cos[\omega_0(t+t_p)]f_l(t+t_g)
\label{e0.1.13}
\end{eqnarray}
%
which is the desired result.

	The significance of the result in (\ref{e0.1.13}) is clear.
The shape of the signal envelope due to $f_l(t)$
is unchanged and shifted in time by $t_g$.  The carrier,
however, is shifted in time by $t_p$ (which in general
is not equal to $t_g$).  Consequently, the overall appearance of
the ouput signal is changed with respect to that of the input
due to the difference in phase shift between the carrier and
the envelope.  This phenomenon is illustrated in 
Figures~\ref{f0.1.1}-\ref{f0.1.5}.  Figure~\ref{f0.1.1} illustrates
%
\input{\Figdir group.1.tex}
\dessin{{\tt exec('group.1\_5.code')} Modulated Exponential Signal}
{f0.1.1}

%
a narrowband signal which consists of a sinusoid modulated by an envelope.
The envelope is an decaying exponential and is displayed in the figure
as the dotted curve.  

	Figure~\ref{f0.1.2} shows the band pass filter
used to filter the signal in Figure~\ref{f0.1.1}.  The filter magnitude
is plotted as the solid curve and the filter phase is plotted as the 
dotted curve.  
%
\input{\Figdir group.2.tex}
\dessin{{\tt exec('group.1\_5.code')} Constant Phase Band Pass Filter}
{f0.1.2}

%
Notice that since the phase is a constant function that $t_g=0$.
The value of the phase delay is $t_p=\pi/2$.  As is expected, the
filtered output of the filter consists of the same signal as the 
input except that the sinusoidal carrier is now phase shifted by $\pi/2$.
This output signal is displayed in Figure~\ref{f0.1.3} as the solid
curve.  For reference the input signal is plotted as the dotted curve.
%
\input{\Figdir group.3.tex}
\dessin{{\tt exec('group.1\_5.code')} Carrier Phase Shift by $t_p=\pi/2$}
{f0.1.3}

%

	To illustrate the effect of the group delay on the filtering
process a new filter is constructed as is displayed in Figure~\ref{f0.1.4}.
%
\input{\Figdir group.4.tex}
\dessin{{\tt exec('group.1\_5.code')} Linear Phase Band Pass Filter}
{f0.1.4}

%
Here the phase is again displayed as the dotted curve.  The group
delay is the slope of the phase curve as it passes through zero in
the pass band region of the filter.  Here $t_g=-1$ and $t_p=0$.
The result of filtering with this phase curve is display in
Figure~\ref{f0.1.5}.  As expected, the envelope is shifted 
but the sinusoid is not shifted within the reference frame of the 
window.  The original input signal is again plotted as the dotted 
curve for reference.
%
\input{\Figdir group.5.tex}
\dessin{{\tt exec('group.1\_5.code')} Envelope Phase Shift by $t_g=-1$}
{f0.1.5}

%
\subsubsection{The Macro {\tt group}}
\index{macro syntax!group@{\tt group}}
\index{group delay!macro syntax}

	As can be seen from the explanation given
in this section, it is preferable that the group delay of a filter 
be constant.  A non-constant group delay tends to cause signal
deformation.  This is due to the fact that the different frequencies which
compose the signal are time shifted by different amounts according
to the value of the group delay at that frequency.  Consequently, it
is valuable to examine the group delay of filters during the design
procedure.  The macro {\tt group} accepts filter parameters in several
formats as input and returns the group delay as output.  The syntax 
of the macro is as follows:
\begin{verbatim}
-->[tg,fr]=group(npts,h)
\end{verbatim}
The group delay {\tt tg} is evaluated in the interval [0,.5) at equally
spaced samples contained in {\tt fr}.   The number of samples is
governed by {\tt npts}.  Three formats can be used for the specification
of the filter.  The filter {\tt h} can be specified
by a vector of real numbers, by a rational polynomial representing 
the z-transform of the filter, or by a matrix polynomial representing a
cascade decomposition of the filter.  The three cases are illustrated below.

	The first example is for a linear-phase filter designed
using the macro {\tt wfir}
\verbatok{\Diary group.1.dia}
\end{verbatim}
as can be seen in Figure~\ref{f0.1.6}
%
\input{\Figdir group.6.tex}
\dessin{{\tt exec('group.6\_8.code')} Group Delay of Linear-Phase Filter}
{f0.1.6}

%
the group delay is a constant, as is to be expected for a linear
phase filter.  The second example specifies a rational polynomial
for the filter transfer function:
\verbatok{\Diary group.2.dia}
\end{verbatim}
The plot in Figure~\ref{f0.1.7} gives the result of this calculation.
%
\input{\Figdir group.7.tex}
\dessin{{\tt exec('group.6\_8.code')} Group Delay of Filter (Rational Polynomial)}
{f0.1.7}

%
Finally, the third example gives the transfer function of the
filter in cascade form.
\verbatok{\Diary group.3.dia}
\end{verbatim}
The result is shown in Figure~\ref{f0.1.8}.  The cascade
realization is known for numerical stability.
%
\input{\Figdir group.8.tex}
\dessin{{\tt exec('group.6\_8.code')} Group Delay of Filter (Cascade Realization)}
{f0.1.8}

%

\subsection{The {\tt group} Macro Code}
\index{macro code!group@{\tt group}}
\index{group delay!macro code}

	The code for the macro {\tt group} is as follows.
\verbatok{\MACROSSIGLIB/group.sci}
\end{verbatim}

\subsection{Appendix: $\Psi$Lab Code Used to Generate Examples}

	The following listing of $\Psi$Lab code was used to
generate the examples of the this section.

\verbatok{\Figdir group.1_5.code}
\end{verbatim}

\section{Sampling}
\label{c2.sampling}
\index{sampling}

	Many signals of interest to scientists
and engineers are continuous in time.  Because digital
computers and digital hardware provide remarkably
flexible signal processing tools
it is often desirable to sample continuous-time signals.
The discrete-time signals which result from sampling
can then be processed by digital signal processing
techniques.  If necessary, the resulting digitally processed signals
can be interpolated back to continuous time signals.
The conversion of analog signals to digital signals
and perhaps back again to analog signals has an effect on the signal
under study.  It is valuable to
understand what effect these procedures have.
The remainder of this section explains in detail the
relationship between continuous and discrete signals.

	To begin, it is useful to examine the 
Fourier transform pairs for continuous and discrete
time signals.  For $x(t)$ and $X(\Omega)$ a continuous time signal and
its Fourier transform, respectively, we have that
%
\begin{equation}
X(\Omega)=\int_{-\infty}^{\infty}x(t)e^{-j\Omega t}dt
\label{e0.1}
\end{equation}
%
%
\begin{equation}
x(t)=\frac{1}{2\pi}\int_{-\infty}^{\infty}X(\Omega)e^{j\Omega t}d\Omega.
\label{e0.2}
\end{equation}
%
For $x(n)$ and $X(\omega)$ a discrete time signal and
its Fourier transform, respectively, we have that
%
\begin{equation}
X(\omega)=\sum_{n=-\infty}^{\infty}x(n)e^{-j\omega n}
\label{e0.3}
\end{equation}
%
%
\begin{equation}
x(n)=\frac{1}{2\pi}\int_{-\pi}^{\pi}X(\omega)e^{j\omega n}d\omega.
\label{e0.4}
\end{equation}
%
The discrete time signal, $x(n)$, is obtained by
sampling the continuous time signal, $x(t)$, at regular
intervals of length $T$ called the sampling period.  
That is,
%
\begin{equation}
x(n)=x(t)|_{t=nT}
\label{e0.5}
\end{equation}
%
We now derive the relationship between the Fourier
transforms of the continuous and discrete time signals.
The discussion follows \cite{oppen}.

	Using (\ref{e0.5}) in (\ref{e0.2}) we have that
%
\begin{equation}
x(n)=\frac{1}{2\pi}\int_{-\infty}^{\infty}X(\Omega)e^{j\Omega nT}d\Omega.
\label{e0.6}
\end{equation}
%
Rewriting the integral in (\ref{e0.6}) as a sum of integrals
over intervals of length $2\pi/T$ we have that
%
\begin{equation}
x(n)=\frac{1}{2\pi}\sum_{r=-\infty}^{\infty}
     \int_{(2\pi r -\pi)/T}^{(2\pi r +\pi)/T}
         X(\Omega)e^{j\Omega nT}d\Omega
\label{e0.7}
\end{equation}
%
or, by a change of variables
%
\begin{equation}
x(n)=\frac{1}{2\pi}\sum_{r=-\infty}^{\infty}
     \int_{-\pi/T}^{\pi/T}
         X(\Omega+\frac{2\pi r}{T})e^{j\Omega nT}e^{j2\pi nr}d\Omega.
\label{e0.8}
\end{equation}
%
Interchanging the sum and the integral in (\ref{e0.8}) and 
noting that $e^{j2\pi nr}=1$ due to the fact that $n$ and $r$
are always integers yields
%
\begin{equation}
x(n)=\frac{1}{2\pi}\int_{-\pi/T}^{\pi/T}
      [\sum_{r=-\infty}^{\infty}
         X(\Omega+\frac{2\pi r}{T})]e^{j\Omega nT}d\Omega.
\label{e0.9}
\end{equation}
%
Finally, the change of variables $\omega=\Omega T$ gives
%
\begin{equation}
x(n)=\frac{1}{2\pi}\int_{-\pi}^{\pi}
      [\frac{1}{T}\sum_{r=-\infty}^{\infty}
         X(\frac{\omega}{T}+\frac{2\pi r}{T})]e^{j\omega n}d\omega
\label{e0.10}
\end{equation}
%
which is identical in form to (\ref{e0.4}).  Consequently,
the following relationship exists between the
Fourier transforms of the continuous and discrete
time signals:
%
\begin{eqnarray}
X(\omega)&=&\frac{1}{T}\sum_{r=-\infty}^{\infty}
         X(\frac{\omega}{T}+\frac{2\pi r}{T})\nonumber\\
         &=&\frac{1}{T}\sum_{r=-\infty}^{\infty}
         X(\Omega+\frac{2\pi r}{T}).
\label{e0.11}
\end{eqnarray}
%

	From (\ref{e0.11}) it can be seen that
the Fourier transform of $x(n)$, $X(\omega)$, is 
periodic with period $2\pi/T$.  The form of $X(\omega)$ consists of
repetitively shifting and superimposing the Fourier transform of 
$x(t)$, $X(\Omega)$, scaled by the factor $1/T$.  For example,
if $X(\Omega)$ is as depicted in Figure~\ref{f00.1}, where the 
%
\input{\Figdir sample.1.tex}
\dessin{{\tt exec('sample.1.code')} Frequency Response $X(\Omega)$}
{f00.1}

%
highest non-zero frequency of $X(\Omega)$ is denoted by $\Omega_c=2\pi f_c$,
then there are two possibilities for $X(\omega)$.  If
$\pi/T>\Omega_c=2\pi f_c$ then $X(\omega)$ is as in Figure~\ref{f0.2},
%
\input{\Figdir sample.2.tex}
\dessin{{\tt exec('sample.2.code')} Frequency Response $x(\omega)$ With No Aliasing}
{f0.2}

%
and, if $\pi/T<\Omega_c=2\pi f_c$, then $X(\omega)$ is as in
Figure~\ref{f0.3}.  That is,
%
\input{\Figdir sample.3.tex}
\dessin{{\tt exec('sample.3.code')} Frequency Response $x(\omega)$ With Aliasing}
{f0.3}

%
to say that if the sampling frequency $f_s=1/T$ is greater
than twice the highest frequency in $x(t)$ then there is
no overlap in the shifted versions of $X(\Omega)$
in (\ref{e0.11}).  However, if $f_s<2f_c$ then the resulting 
$X(\omega)$ is composed of overlapping versions of $X(\Omega)$.

	The sampling rate $T=1/(2f_c)$ is the well known
Nyquist sampling rate and any signal sampled at a rate higher 
than the Nyquist rate retains all of the information
that was contained in the original unsampled signal.
It can be concluded that sampling can retain
or alter the character of the original continuous time signal.
If sampling is performed at more than twice the highest frequency
in $x(t)$ then the signal nature is retained.  Indeed, the
original signal can be recuperated from the sampled signal
by low pass filtering (as is demonstrated below).  However,
if  the signal is undersampled this results in a signal
distortion known as aliasing.

	To recuperate the original analog signal from
the sampled signal it is assumed that $\Omega_c<\pi/T$ (i.e., that
the signal is sampled at more than twice its highest 
frequency).  Then from (\ref{e0.11})
%
\begin{equation}
X(\Omega)=TX(\omega)
\label{e0.12}
\end{equation}
%
in the interval $-\pi/T\le\Omega\le\pi/T$.  Plugging (\ref{e0.12})
into (\ref{e0.2}) yields
%
\begin{equation}
x(t)=\frac{1}{2\pi}\int_{-\pi/T}^{\pi/T}TX(\omega)e^{j\Omega t}d\Omega.
\label{e0.13}
\end{equation}
%
Replacing $X(\omega)$ by (\ref{e0.3}) and using (\ref{e0.5}) we have that
%
\begin{equation}
x(t)=\frac{T}{2\pi}\int_{-\pi/T}^{\pi/T}[
      \sum_{-\infty}^{\infty}x(nT)e^{-j\Omega nT}]e^{j\Omega t}d\Omega.
\label{e0.14}
\end{equation}
%
Interchanging the sum and the integral gives
%
\begin{equation}
x(t)=\sum_{-\infty}^{\infty}x(nT)
     [\frac{T}{2\pi}\int_{-\pi/T}^{\pi/T}
    e^{j\Omega(t- nT)}d\Omega].
\label{e0.15}
\end{equation}
%
The expression in brackets in (\ref{e0.15}) can be recognized as
a time shifted inverse Fourier transform of a low pass filter with
cut-off frequency $\pi/T$.  Consequently, (\ref{e0.15}) is a
convolution between the sampled signal and a low pass filter,
as was stated above.

	We now illustrate the effects of aliasing.
Since square integrable functions can always be decomposed as a sum
of sinusoids the discussion is limited to a signal which is 
a cosine function.  The results of  what happens to a cosine signal
when it is undersampled is directly extensible to more complicated 
signals.

	We begin with a cosine signal as is illustrated in 
Figure~\ref{f0.4}.
%
\input{\Figdir sample.4.tex}
\dessin{{\tt exec('sample.4.code')} Cosine Signal}
{f0.4}

%
The cosine in Figure~\ref{f0.4} is actually a sampled signal which
consists of 5000 samples.  One period of the cosine in the figure
is 200 samples long, consequently, the Nyquist sampling rate requires
that we retain one sample in every 100 to retain the character of the
signal.  By sampling the signal at a rate less than the Nyquist rate
it would be expected that aliasing would occur.  That is, it would
be expected that the sum of two cosines would be evident in the 
resampled data.  Figure~\ref{f0.5} illustrates the data resulting
from sampling the cosine in Figure~\ref{f0.4} at a rate of ones every
105 samples.
%
\input{\Figdir sample.5.tex}
\dessin{{\tt exec('sample.5.code')} Aliased Cosine Signal}
{f0.5}

%
As can be seen in Figure~\ref{f0.5}, the signal is now the sum
of two cosines which is illustrated by the beat signal illustrated
by the dotted curves.

\section{Decimation and Interpolation}
\index{decimation}
\index{interpolation}

\subsection{Introduction}

	There often arises a need to change the sampling
rate of a digital signal.  For example, one may have a digital
signal which is sampled at 10kHz (i.e. 10,000 samples/second)
and would like to filter this signal with hardware 
which accepts data at only an 8kHz rate.  
To understand how this may be accomplished 
it is first useful to understand the relationship
between the Fourier transform of a continuous-time signal, $x(t)$,
and the Fourier transform of the discrete-time signal, $x(nT)$,
obtained by sampling $x(t)$ with frequency $1/T$.
The two Fourier transforms are defined, respectively, in (\ref{e1a})
and (\ref{e1b}) below
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
\hat{X}(\omega)=\int_{-\infty}^{\infty}x(t)e^{-j\omega t}dt
\label{e1a}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
X(e^{j\omega T})=\sum_{n=-\infty}^{\infty}x(nT)e^{-j\omega T}.
\label{e1b}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
The relationship between these two transforms is (see Section 1.7
of {\em Digital Signal Processing}, A.V. Oppenheim and R.W. Schafer,
Prentice-Hall, Inc., 1975.)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
X(e^{j\omega T})=\frac{1}{T}\sum_{r=-\infty}^{\infty}
\hat{X}(\frac{j\omega}{T}+\frac{j2\pi r}{T}).
\label{e1c}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
Figure~\ref{f0.1}
illustrates the magnitude of the Fourier 
transform $\hat{X}(\omega)$ of a signal $x(t)$. 
Figure~\ref{f1.1} shows two periods of the associated Fourier 
transform $X(e^{jwT})$
of $x(nT)$ where the sampling frequency was taken
to be the Nyquist rate.  As indicated by (\ref{e1c}), 
the magnitude of $X(e^{jwT})$ with
respect to the magnitude of $\hat{X}(\omega)$ is scaled by $1/T$.
%
\input{\Figdir intdec.1.tex}
\dessin{{\tt exec('intdec.1\_4.code')} Fourier Transform of a Continuous Time Signal}
{f0.1}

%
%
\input{\Figdir intdec.2.tex}
\dessin{{\tt exec('intdec.1\_4.code')} Fourier Transform of the Discrete Time Signal}
{f1.1}


%
Furthermore, $X(e^{jwT})$ is periodic with period $2\pi/T$.  If
we take $1/T\ge\pi/\Omega$, where $\Omega$ is the highest
non-zero frequency of $X(\omega)$, then no aliasing occurs in sampling the 
continuous-time signal.  When this is the case one can, in principle,
perfectly reconstruct the original continuous-time signal $x(t)$
from its samples $x(nT)$ using
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
x(t)=\sum_{n=-\infty}^{\infty}
x(nT)\frac{\sin[(\pi/T)(t-nT)]}{(\pi/T)(t-nT)}.
\label{e1d}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
Consequently, one could obtain $x(t)$ sampled at a different sampling
rate $T'$ from the sampled signal $x(nT)$ by using (\ref{e1d}) 
to reconstruct $x(t)$ and
then resampling.  In practice, however, this is impractical.  It is
much more convenient to keep all operations in the digital domain
once one already has a discrete-time signal.

	The macro {\tt intdec} accomplishes
a sampling rate change by sequentially using two different 
digital procedures.
The first procedure, interpolation, takes the input
signal and produces an output signal which is sampled at a 
rate $L$ (an integer) times more frequently than the input.
The second procedure,  decimation,
takes the input signal and produces an output signal which 
is sampled at a rate $M$ (also an integer) times less frequently than the 
input.  By using the two
procedures in tandem one can achieve any desired rational
change in sampling rate.

\subsection{Interpolation}
\index{interpolation}

	When a discrete-time signal is obtained by sampling a
continuous-time signal, x(t), the Fourier transform of this discrete-time
signal implicitly includes the sampling period, $T$
(and, consequently, the sampling frequency, $f=1/T$), of the signal.
Often the sampling period $T$ is normalized to unity
because it is of no consequence in the ensuing analysis.
However, if one wants to change the sampling rate
(and, thus, the sampling period) it is valuable to keep
the explicit representation of $T$ in (\ref{e1b}).

	In interpolating the input signal by the integer $L$
we wish to obtain a new signal $x(nT')$ where $x(nT')$ would
be the signal obtained if we had originally 
sampled the continuous-time signal $x(t)$ at the rate $1/T'=L/T$.
If the original signal is bandlimited and the sampling rate 
$f=1/T$ is greater than twice the
highest frequency of $x(t)$ then it can be expected that
the new sampled signal $x(nT')$ (sampled at a rate of
$f'=1/T'=L/T=Lf$) could be obtained directly from the discrete
signal $x(nT)$.
In fact, such a procedure exists and is outlined below.

	An interpolation of $x(nT)$ to $x(nT')$ where $T'=T/L$
can be found by inserting $L-1$ zeros between each element of the
sequence $x(nT)$ and then low pass filtering.  To see this we construct
the new sequence $v(nT')$ by putting $L-1$ zeros between the 
elements of $x(nT)$
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
v(nT')=\left\{ \begin{array}{ll}
                     x(nT/L), & \mbox{$n=0,\pm L,\pm 2L,\ldots$}\\
                     0, & \mbox{otherwise}.
                \end{array}
       \right.
\label{e2.1}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
Since $T'=T/L$, $v(nT')$ is sampled $L$ times more frequently than 
$x(nT)$.  The Fourier transform of (\ref{e2.1}) yields
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{eqnarray}
V(e^{j\omega T'}) 
         &=& \sum_{n=-\infty}^{\infty}v(nT')e^{-j\omega nT'}\nonumber\\
         &=& \sum_{n=-\infty}^{\infty}x(nT)e^{-j\omega nLT'}\nonumber\\
         &=& \sum_{n=-\infty}^{\infty}x(nT)e^{-j\omega nT}\nonumber\\
                  &=& X(e^{j\omega T}).
\label{e3.1}
\end{eqnarray}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
From (\ref{e3.1}) it can be seen that $V(e^{j\omega T'})$ is periodic with
period $2\pi/T$ and, also, period $2\pi/T'=2\pi L/T$.  This fact is 
illustrated in Figure~\ref{f2.1} where $L=3$.  Since the sampling frequency of 
$V$ is $1/T'$ we see that by filtering $v(nT')$ with a low
%
\input{\Figdir intdec.3.tex}
\dessin{{\tt exec('intdec.1\_4.code')} Fourier Transform of $v(nT')$}
{f2.1}

%
pass filter with cut-off frequency at $\pi/T$ we obtain
exactly the interpolated sequence, $x(nT')$, which we seek
(see Figure~\ref{f3.1}), except for a scale factor of $L$ (see (\ref{e1c})).
%
\input{\Figdir intdec.4.tex}
\dessin{{\tt exec('intdec.1\_4.code')} Fourier Transform of $x(nT')$}
{f3.1}

%

\subsection{Decimation}
\index{decimation}

	Where the object of interpolation is to obtain $x(nT')$
from $x(nT)$ where $T'=L/T$, the object of decimation is
to find $x(nT'')$ from $x(nT)$ where $T''=MT$, $M$ an integer.
That is, $x(nT'')$ should be equivalent to a sequence obtained
by sampling $x(t)$ $M$ times less frequently than that for $x(nT)$.
Obviously this can be accomplished by keeping only every $M^{th}$
sample of $x(nT)$.  However, if the sampling frequency $1/T$ is
close to the Nyquist rate then keeping only every $M^{th}$
sample results in aliasing.  Consequently, low pass filtering
the sequence $x(nT)$ before discarding $M-1$ of each $M$ points is 
advisable.  Assuming that the signal $x(nT)$ is
sampled at the Nyquist rate, the cut-off frequency of the low pass filter
must be at $\pi/(MT)$.

\subsection{Interpolation and Decimation}

	To change the sampling rate of a signal by a non-integer
quantity it suffices to perform a combination of the interpolation
and decimation operations.  Since both operations use a low-pass filter they
can be combined as illustrated in the block diagram of Figure~\ref{f44.1}.
The macro {\tt intdec} begins by designing a low-pass filter for the 
diagram illustrated in the figure.  It accomplishes this by using
the {\tt wfir} filter design macro.  This is followed by taking the
Fourier transform of both the input signal and the low-pass filter
(whose magnitude is first scaled by $L$)
by using the {\tt fft} macro.  Care must be taken to obtain a linear
convolution between the two sequences by adding on an appropriate number
of zeros to each sequence before the FFT is performed.  After multiplying
the two transformed sequences together an inverse Fourier transform
is performed.  Finally, the output is obtained by discarding $M-1$
of each $M$ points.
%
\begin{figure}[tb]
\center{
\begin{picture}(360,204)

\put(10,120){\makebox(0,0){$x(nT)$}}
\put(25,120){\vector(1,0){10}}
\put(35,100){\framebox(65,40){\shortstack{Put\\L-1 Zeros\\Between\\Each Sample}}}
\put(100,120){\vector(1,0){15}}

\put(115,100){\framebox(65,40){LPF}}
\put(180,120){\vector(1,0){15}}

\put(195,100){\framebox(65,40){\shortstack{Discard\\M-1 of Every\\M Samples}}}
\put(260,120){\vector(1,0){10}}
\put(300,120){\makebox(0,0){$x(nMT/L)$}}

\end{picture}}
\caption{ Block Diagram of Interpolation and Decimation}
\label{f44.1}
\end{figure}
%
The cut-off frequency of the low pass filter is $\pi/T$
if $L>M$ and is $(L\pi)/(MT)$ if $L<M$.

	The practical implementation of the interpolation and decimation
procedure is as follows.
If the length of the input is $N$ then after putting $L-1$ zeros
between each element of $x$ the resulting sequence will be of length
$(N-1)L+1$.  This new sequence is then lengthened by $K-1$ zeros
where $K$ is the length of the low pass filter.  This lengthening is
to obtain a linear convolution between the input and the low pass
filter with the use of the FFT.  The cut-off frequency
of the low pass filter is chosen to be $(.5N)/[(N-1)L+K]$ if $L>M$
and $(.5NL)/(M[(N-1)L+K])$ if $L<M$.  
The FFT's of the two modified sequences are multiplied
element by element and then are inverse Fourier transformed.
The resulting sequence is of length $(N-1)L+K$.  To obtain a
sequence of length of $(N-1)L+1$ elements, $(K-1)/2$ elements
are discarded off of each end.  Finally, $M-1$ out of every
$M$ elements are discarded to form the output sequence.

\subsection{How to use {\tt intdec}}
\index{macro syntax!intdec@{\tt intdec}}
\index{interpolation-decimation!macro syntax}
\label{s4.1}

	The syntax for the macro {\tt intdec} is as follows:
\begin{verbatim}
--> [y]=intdec(x,l,m,forder)
\end{verbatim}
where {\tt x} is the input sequence  
whose sampling rate is to be changed by
{\tt l/m}, {\tt y} is the output sequence with the new sampling rate, and 
{\tt forder} is the length of the low-pass filter to be used in {\tt intdec}.
 
\subsection{Example}
\index{interpolation-decimation!example}

	Here we take a 50-point sequence assumed to be sampled at
a 10kHz rate and change it to a sequence sampled at 16kHz.  Under
these conditions we take $L=8$ and $M=5$.
The sequence, $x(nT)$, is illustrated
in Figure~\ref{f55.1}.  The discrete Fourier transform of x(nT) is shown in
%
\input{\Figdir intdec.5.tex}
\dessin{{\tt exec('intdec.5\_10.code')} The Sequence $x(nT)$}
{f55.1}

%
%
\input{\Figdir intdec.6.tex}
\dessin{{\tt exec('intdec.5\_10.code')} The DFT of $x(nT)$}
{f6.1}

%
Figure~\ref{f6.1}.  As can be seen, $x(nT)$ is a bandlimited sequence.
A new sequence $v(nT')$ is created by putting 7 zeros 
between each element of $x(nT)$.
We use a Hamming windowed lowpass filter of length 33 (Figure~\ref{f7.1})
%
\input{\Figdir intdec.7.tex}
\dessin{{\tt exec('intdec.5\_10.code')} Low Pass Filter}
{f7.1}

%
to filter $v(nT')$.  The discrete Fourier transform of $v(nT')$ is
illustrated in
Figure~\ref{f8.1}.  As is to be expected, the Fourier transform of $v(nT')$
looks like the Fourier transform of $x(nT)$ repeated 8 times.
%
\input{\Figdir intdec.8.tex}
\dessin{{\tt exec('intdec.5\_10.code')} DFT of $v(nT')$}
{f8.1}

%
The result of multiplying the magnitude response 
of the filter with that of the
sequence $v(nT')$ is shown in Figure~\ref{f9.1}.  Since the low pass filter
is not ideal the resulting filtered sequence has some additional high frequency
energy in it (i.e., the small lobes seen in Figure~\ref{f9.1}).
%
\input{\Figdir intdec.9.tex}
\dessin{{\tt exec('intdec.5\_10.code')} Filtered Version of $V$}
{f9.1}

%
Finally, after taking the inverse discrete Fourier transform and discarding
4 out of every 5 samples we obtain the sequence illustrated in
Figure~\ref{f10.1}.
%
\input{\Figdir intdec.10.tex}
\dessin{{\tt exec('intdec.5\_10.code')} Sequence $x(nMT/L)$}
{f10.1}

%

\subsection{Macro Code}
\index{macro code!intdec@{\tt intdec}}
\index{interpolation-decimation!macro code}

\verbatok{\MACROSSIGLIB/intdec.sci}
\end{verbatim}

\section{The DFT and the FFT}
\index{DFT}
\index{FFT}

\subsection{Introduction}

	The FFT (``Fast Fourier Transform'') is a computationally 
efficient algorithm for calculating the DFT (''Discrete Fourier Transform'')
of finite length discrete time sequences.
  Calculation of the DFT from its definition
requires order $N^2$ multiplications whereas the FFT requires
order $N\log_2N$ multiplications.
In this section we discuss several uses of the DFT and some examples
of its calculation using the FFT primitive in $\Psi$Lab.

	We begin with the definition of the DFT
for a finite length sequence, $x(n)$, of length $N$,
%
\begin{equation}
X(k)=\sum_{n=0}^{N-1}x(n)e^{-j\frac{2\pi}{N}nk}.
\label{e.fft.1}
\end{equation}
%
A careful examination of (\ref{e.fft.1}) reveals that
$X(k)$, the DFT of $x(n)$, is periodic with period $N$ (due to the fact
that for fixed $n$ the term $\exp(-j2\pi nk/N)$ is periodic
with period $N$).  That $X(k)$ is periodic
also follows from the fact that (\ref{e.fft.1}) can be interpreted
as samples of the $z$-transform of $x(n)$ at $N$ equally spaced
spaced samples on the unit circle.
For reasons of symmetry, the DFT is defined to consist of the $N$
distinct points of $X(k)$ for $k=0,1,\ldots,N-1$.

	The $N$ points of the sequence $x(n)$ can be recovered from
$N$ points of $X(k)$.  This recovery is called the inverse
DFT and takes the form
%
\begin{equation}
x(n)=\frac{1}{N}\sum_{k=0}^{N-1}X(k)e^{j\frac{2\pi}{N}nk}.
\label{e.fft.2}
\end{equation}
%
It should be noted that (\ref{e.fft.2}) yields a periodic sequence in $n$ 
of period $N$ and that it is the $N$
values of $x(n)$, $n=0,1,\ldots,N-1$ which yield
the desired finite length sequence.

	In order to better understand the DFT, one can think of 
(\ref{e.fft.1}) as being represented by a matrix computation
%
\begin{equation}
\left[\begin{array}{c}
X(1)\\X(2)\\\vdots\\X(N-1)\end{array}\right]
=
\left[\begin{array}{ccccc}
1&1&1&\cdots&1\\
1&e^{-j\frac{2\pi}{N}}&e^{-j\frac{4\pi}{N}}&\cdots&e^{-j\frac{2(N-1)\pi}{N}}\\
1&e^{-j\frac{4\pi}{N}}&e^{-j\frac{8\pi}{N}}&\cdots&e^{-j\frac{4(N-1)\pi}{N}}\\
\vdots&\vdots&\vdots&&\vdots\\
1&e^{-j\frac{2(N-1)\pi}{N}}&e^{-j\frac{4(N-1)\pi}{N}}&\cdots&e^{-j\frac{(N-1)^2\pi}{N}}\\
\end{array}\right]
\left[\begin{array}{c}
x(1)\\x(2)\\\vdots\\x(N-1)\end{array}\right].
\label{e.fft.3}
\end{equation}
%
The inverse DFT can be calculated in a similar fashion, where the
matrix used is the Hermitian transpose of that in (\ref{e.fft.3})
times a factor of $1/N$.
From (\ref{e.fft.3}) it can be seen that the DFT requires order
$N^2$ multiplications.  An example of a $\Psi$Lab macro which calculates
the DFT is as follows:
\verbatok{\MACROSSIGLIB/dft.sci}
\end{verbatim}

	An important property of the DFT has to do with the
inverse DFT of the product of two transformed sequences.
Taking $x(n)$ and $h(n)$ to be two sequences of length $N$
the DFT's of these two sequences are
%
\begin{equation}
X(k)=\sum_{n=0}^{N-1}x(n)e^{-j\frac{2\pi}{N}nk}
\label{e.fft.4}
\end{equation}
%
and
%
\begin{equation}
H(k)=\sum_{n=0}^{N-1}h(n)e^{-j\frac{2\pi}{N}nk}.
\label{e.fft.5}
\end{equation}
%
Taking the inverse DFT of the product $Y(k)=H(k)X(k)$ yields
%
\begin{eqnarray}
y(n)&=&\frac{1}{N}\sum_{k=0}^{N-1}H(k)X(k)e^{j\frac{2\pi}{N}nk}\nonumber\\
    &=&\sum_{m=0}^{N-1}h(m)\sum_{r=0}^{N-1}x(r)\frac{1}{N}\sum_{k=0}^{N-1}e^{j\frac{2\pi}{N}(n-m-r)k}\nonumber\\
    &=&\sum_{m=0}^{N-1}h(m)x(n-m)
\label{e.fft.6}
\end{eqnarray}
%
where the last equality follows from the fact that
%
\begin{equation}
\frac{1}{N}\sum_{k=0}^{N-1}e^{j\frac{2\pi}{N}(n-m-r)k}=\left\{
   \begin{array}{cc}
                1,&r=n-m\\
                0,&\mbox{otherwise}\end{array}\right..
\label{e.fft.7}
\end{equation}
%
In (\ref{e.fft.6}) it should be noted that when the argument of
$x$ is outside of the the range $[0,N-1]$ that the value of $x$
is obtained from evaluating the argument modulo $N$ (which follows
from the periodicity of the $x$ obtained by using the inverse DFT).
This property of the DFT is important in the calculation of convolutions
and more on this subject is explained in the section on convolution.

	It is equally possible to compute a multi-dimensional DFT.
For a multi-dimensional sequence $x(n_1,n_2,\ldots,n_M)$ the multi-dimensional
DFT is defined by
%
\begin{eqnarray}
X(k_1,k_2,\ldots,k_M)&=&\nonumber\\
\sum_{n_1=1}^{N_1-1}e^{-j\frac{2\pi}{N_1}n_1k_1}\sum_{n_2=1}^{N_2-1}e^{-j\frac{2\pi}{N_2}n_2k_2}&\cdots&\sum_{n_M=1}^{N_M-1}e^{-j\frac{2\pi}{N_M}n_Mk_M}x(n_1,n_2,\ldots,n_M).
\end{eqnarray}
%
The inverse multi-dimensional DFT is analogous to the calculation above
 with a change of sign for the complex exponentials
and a factor of $1/(N_1N_2\cdots N_M)$.

	The DFT and the inverse DFT are
important for many signal processing applications.
The most direct application of the DFT is
the calculation of the spectrum of finite
length discrete signals.  In fact, the DFT 
is a projection of $x(n)$ onto the orthogonal
basis consisting of the $N$ complex
exponentials $\exp(-j2\pi nk/N)$ indexed by $k$.
Consequently, the DFT is a tool which gives
information about the periodic components of $x(n)$.

	A very important application of the DFT
is for calculating the interaction of discrete
signals and discrete linear systems.  When a
discrete signal is introduced to the input of a
linear system, the resulting output is a convolution
of the input signal with the impulse response of
the linear system.  The convolution operation, which
is discussed in more detail in the section
on convolution, requires order $N^2$ multiplications where
the signal and system impulse response are both of length $N$.
Calculating the convolution by FFT requires order
$N\log_2N$ multiplications.

	The FFT algorithm is a computationally
efficient way of calculating the DFT.  The computational
savings realized by the FFT are obtained by
exploiting certain symmetries and periodicities which exist in the
calculation of the DFT.  To show how the FFT
exploits these properties of the DFT we assume that $N=2^{\gamma}$
for $\gamma$ a positive integer and we calculate
the DFT of $x(n)$ as follows
%
\begin{eqnarray}
X(k)&=&\sum_{n=0}^{N-1}x(n)e^{-j\frac{2\pi}{N}nk}\nonumber\\
    &=&\sum_{n=even}x(n)e^{-j\frac{2\pi}{N}nk}+\sum_{n=odd}x(n)e^{-j\frac{2\pi}{N}nk}\nonumber\\
    &=&\sum_{r=0}^{\frac{N}{2}-1}x(2r)e^{-j\frac{2\pi}{N/2}rk}+e^{j\frac{2\pi}{N}k}\sum_{r=0}^{\frac{N}{2}-1}x(2r+1)e^{-j\frac{2\pi}{N/2}rk}
\label{e.fft.8}
\end{eqnarray}
%
where the final equality is obtained by making
the change of variables $n=2r$.  The expression in (\ref{e.fft.8})
is composed of a sum of two $N/2$ point DFT's, 
one for the $N/2$ point sequence $x(0)$, $x(2)$, $\ldots$, $x(N-2)$,
and the other for the $N/2$ point sequence $x(1)$, $x(3)$, $\ldots$, $x(N-1)$.
An addition to the two $N/2$ point DFT's in (\ref{e.fft.8}), the
calculation also requires $N$ additional multiplications
for the $N$ terms $\exp(j2\pi k/N)$, $k=0,1,\ldots,N-1$.

	The purpose of calculating the DFT as in 
(\ref{e.fft.8}) is that a computational savings has been realized.
As has already been shown the calculation of the $N$ point DFT
from (\ref{e.fft.1}) requires order $N^2$ multiplications.  Since
(\ref{e.fft.8}) requires the calculation of two $N/2$ point DFT's
plus $N$ additional multiplications, the computational load
is of order $2(N/2)^2+N=N^2/2+N$.  For $\gamma>1$ 
(i.e., for $N\ge4$) we have realized a computational savings
by using (\ref{e.fft.8}).  Furthermore, the operation in (\ref{e.fft.8})
can be repeated in that each of the $N/2$ point
DFT's can be split into two $N/4$ point DFT's
plus $N$ additional multiplications.
This yields a computational complexity of $4(N/4)^2+2N$ multiplications.
Continuing in this way $\gamma=\log_2N$ times, the final result
is an algorithm with computational complexity of $N\log_2N$ multiplications.

	The above discussion of the computational advantages of the FFT
is based on the assumption that $N=2^{\gamma}$.  Similar developments
of the FFT can be derived based on any prime factorization of $N$.
The more prime factors $N$ has the greater computational efficiency
can be obtained in using the FFT.  In fact, it may be useful in some
applications to artificially extend the length of a sequence (by adding on
zeros) in order that the length of the sequence will be more factorable.
The FFT primitive in $\Psi$Lab automatically accounts for the prime
factorization of the sequence length.  The next section describes 
how to use the {\tt fft} primitive in $\Psi$Lab.

\subsection{How to Use the {\tt fft} Primitive}
\index{FFT!macro syntax}
\index{macro syntax!fft@{\tt fft}}

	There are two formats for the {\tt fft} primitive in $\Psi$Lab.
The first format accepts a vector or matrix as input and returns,
respectively, a vector or matrix representing the one-dimensional
or two-dimensional  DFT, respectively, of the input.
The second format performs a multi-dimensional DFT for sequences of
dimension greater than or equal to three.  

	The first format has
the syntax
\begin{verbatim}
-->[xk]=fft(x,flag);
\end{verbatim}
where {\tt x} is the input vector or matrix, {\tt flag} indicates
whether the operation is to be a DFT of an inverse DFT, and {\tt xk}
is the resulting output.  When $\mtt{flag}=-1$ the operation is the DFT
and when $\mtt{flag}=1$ the operation is the inverse DFT.

	The second format is used for the calculation of multi-dimensional
DFT's where the dimension is greater than or equal to three.  In this format
it is necessary to call the primitive a number of times equal to
the number of dimensions of the signal.  The syntax of a single
call to {\tt fft} in this format is
\begin{verbatim}
-->[xk]=fft(x,flag,dim,incr);
\end{verbatim}
Here it is assumed that {\tt x} is a vector which contains all the
entries of a multi-dimensional signal $x(n_1,n_2,\ldots,n_M)$.  
Assuming that the above call is being made for the dimension $n_k$,
$1\le k\le M$,
the values of {\tt dim} and {\tt incr} indicate to the {\tt fft}
primitive how the entries of $x$ are arranged in the vector {\tt x}.
That is, the value of {\tt dim} indicates the length of the $n_k$ dimension
and the value of {\tt incr} indicates the increment between subsequent
values of $x$ in {\tt x}.
Thus, for example, if $x(n_1,n_2,n_3)$ is 3-dimensional 
where $n_1=0,1$, $n_2=0,1,2$, $n_3=0,1$ 
and {\tt x} has the entries
$$
\mtt{ x}=[x_{000}, x_{100}, x_{010}, x_{110}, x_{020}, x_{120}, 
x_{001}, x_{101}, x_{011}, x_{111}, x_{021}, x_{121}]
\nonumber
$$
then {\tt dim} takes the subsequent values $[2,3,2]$ while
{\tt incr} takes the values $[1,2,6]$ for the three consecutive
calls to {\tt fft}.

	If the user can arrange the vector {\tt x} in the logical
order proposed above, then there is a relationship between the values
of {\tt dim} and {\tt incr}.  The following macro simplifies the
use of {\tt fft} in the multi-dimensional case by making use of this
relationship between {\tt dim} and {\tt incr}.
\verbatok{\MACROSSIGLIB/mfft.sci}
\end{verbatim}
The inputs to this macro are the data vector {\tt x} (organized as in
the example above), {\tt flag} which indicates whether the FFT or
the inverse FFT is to calculated, and {\tt dim} which is now a vector
with the individual lengths of each dimension of $x$.  The output
{\tt xk} is organized in the same manner as {\tt x}.

\subsection{Examples Using the {\tt fft} Primitive}
\index{FFT!examples}

	Two examples are presented in this section.  The first 
example illustrates how to use the {\tt fft} primitive to calculate
a one-dimensional DFT.  The second example calculates a three-dimensional
DFT.  

	For the first example, data from a cosine function is passed
to the {\tt fft} primitive.  The data is created as follows
\verbatok{\Figdir fft.1.code}
\end{verbatim}
\verbatok{\Figdir fft.2.code}
\end{verbatim}
The cosine data is displayed in Figure~\ref{f.fft.1}.
%
\input{\Figdir fft.1.tex}
\dessin{{\tt exec('fft.1.code')} Cosine Signal}{f.fft.1}
resulting output from the {\tt fft} primitive is displayed
in Figure~\ref{f.fft.2}.  Figure~\ref{f.fft.2} displays the magnitude
%
\input{\Figdir fft.2.tex}
\dessin{{\tt exec('fft.2.code')} DFT of Cosine Signal}{f.fft.2}
he DFT.  Note, however, that since the cosine function is
an even symmetric function, the DFT of the cosine is strictly real
and, thus, the magnitude and the real part of the DFT are the
same.
Furthermore, since we are calculating a 64-point DFT of a cosine
with frequency $2\pi/16$ it is expected that the DFT should have
peaks at $k=4$ and $k=60$.  This follows from the fact that 
the value $k=64$ of the DFT corresponds to  a frequency of $2\pi$ 
and, consequently, the value $k=4$ must correspond to the frequency
$2\pi/16$, which is the frequency of the signal under examination.

	The second example calculates the DFT of a three-dimensional
signal.  The calculation proceeds as follows.
\verbatok{\Diary fft.1.dia}
\end{verbatim}
In the above series of calculations the signal $y$ is three-dimensional
and is represented by the two matrices {\tt y1} and {\tt y2}.  The first
dimension of $y$ are the rows of the matrices, the second dimension
of $y$ are the columns of the matrices, and the third dimension of $y$
are the sequence of matrices represented by {\tt y1} and {\tt y2}.
The signal $y$ is represented by the vector {\tt y} which is in
vector form.  The DFT of $y$ is calculated using the macro {\tt mfft}
where $\mtt{flag}=-1$ and $\mtt{dim}=[2\ 3\ 2]$.  Naturally, the DFT
of the three-dimensional signal is itself three-dimensional.  The result
of the DFT calculation is represented by the two matrices {\tt yf1}
and {\tt yf2}.

\section{Convolution}
\index{convolution}

\subsection{Introduction}

	Convolution is a mathematical operation which
is defined for continuous and discrete functions.  The
convolution operation is important for the
analysis of linear systems.  Furthermore, the convolution
operation becomes a multiplication when this operation
is represented in the Fourier transform domain.  Consequently,
convolutions can be calculated efficiently using the  FFT.

	Given two continuous time functions $x(t)$ and $h(t)$,
a new continuous time function can be obtained by
convolving $x(t)$ with $h(t)$.  The convolution is
defined by
%
\begin{equation}
y(t)=\int_{-\infty}^{\infty}h(t-u)x(u)du.
\label{e.conv.1}
\end{equation}
%
Consequently, to obtain the value of $y(t)$ at time
$t_0$, the convolution operation reverses the function $h(t)$
in time (i.e., forming $h(-u))$ and then shifts the time
reversed $h(t)$ by the constant $t_0$ which yields the function
$h(t_0-u)$.  This new function is then multiplied by $x(u)$ and intergrated
from $-\infty$ to $\infty$ to obtain $y(t_0)$.

	An analogous convolution operation
can be defined for discrete time functions.  Letting
$x(n)$ and $h(n)$ represent discrete time functions, a
new function, $y(n)$ can be obtained by the discrete time convolution
operation
%
\begin{equation}
y(n)=\sum_{k=-\infty}^{\infty}h(n-k)x(k).
\label{e.conv.2}
\end{equation}
%

	Convolution is important in the analysis
of linear systems.  For example, if $h(t)$ represents the
impulse response of a linear, time-invariant system and if $x(t)$ 
is an input to this system then the output of the system, $y(t)$,
can be calculated as the convolution
between $x(t)$ and $h(t)$ (i.e., as in (\ref{e.conv.1}).
Figure~\ref{f.conv.1} illustrates how convolution is related to 
linear systems.
%
\begin{figure}[tb]
\center{
\begin{picture}(360,204)

\put(65,120){\makebox(0,0){$x(t)$}}
\put(80,120){\vector(1,0){35}}

\put(115,100){\framebox(65,40){$h(t)$}}
\put(180,120){\vector(1,0){35}}

\put(270,120){\makebox(0,0){$y(t)=\int h(t-u)x(u)du$}}

\end{picture}}
\caption{ Convolution Performed by Linear System}
\label{f.conv.1}
\end{figure}
%
If the system, $h(t)$ is causal (i.e., $h(t)=0$ for $t<0$)
and, in addition, the signal $x(t)$ is applied to the system at
time $t=0$, then (\ref{e.conv.1}) becomes
%
\begin{equation}
y(t)=\int_{0}^{t}h(t-u)x(u)du.
\label{e.conv.3}
\end{equation}
%
Similarly, for $h(n)$ a time invariant, causal, discrete linear
system
with input $x(n)$ starting at time $n=0$, 
the output $y(n)$ is the convolution
%
\begin{equation}
y(n)=\sum_{k=0}^{n}h(n-k)x(k).
\label{e.conv.4}
\end{equation}
%

	An important property of the convolution operation is that
the calculation can be effected by using Fourier transforms.
This is due to the fact that convolution in the time domain is equivalent
to multiplication in the frequency domain.  
Let $X(\omega)$, $H(\omega)$, and $Y(\omega)$ represent
the Fourier transforms of $x(t)$, $h(t)$, and $y(t)$, respectively, where
%
\begin{equation}
Y(\omega)=\int_{-\infty}^{\infty}y(t)e^{-j\omega t}dt.
\label{e.conv.6}
\end{equation}
%
If the relationship in (\ref{e.conv.1}) is valid, then it also follows that
%
\begin{equation}
Y(\omega)=H(\omega)X(\omega).
\label{e.conv.5}
\end{equation}
%

	There is an analogous relationship between the Fourier transform
and convolution for discrete time signals.  
Letting
$X(e^{j\omega})$, $H(e^{j\omega})$, and $Y(e^{j\omega})$ be the 
Fourier transforms of $x(n)$, $h(n)$, and $y(n)$, respectively, where,
for example
%
\begin{equation}
Y(e^{j\omega})=\sum_{n=-\infty}^{\infty}y(n)e^{-j\omega n}
\label{e.conv.8}
\end{equation}
%
we have that
the discrete time convolution operation can be represented in the
Fourier domain as
%
\begin{equation}
Y(e^{j\omega})=H(e^{j\omega})X(e^{j\omega}).
\label{e.conv.7}
\end{equation}
%

	
	The fact that convolution in the time domain is equivalent to
multiplication in the frequency domain means that the 
calculations in (\ref{e.conv.1}) and (\ref{e.conv.2}) can be
calculated (at least approximately) by a computationally efficient
algorithm, the FFT.
The FFT is useful for calculating convolutions when the relevant
functions can be represented by discrete, finite length sequences.
This is accomplished by calculating the inverse DFT 
of the product of the DFT's of $x(n)$ and $h(n)$.  Of course, all
DFT calculations are accomplished using the FFT algorithm.
Care must be taken to ensure that the resulting calculation is
a linear convolution (see the section on the DFT and the FFT).  
The linear convolution is accomplished by adding enough zeros onto
the two sequences so that the circular convolution accomplished by
the DFT is equivalent to a linear convolution.

	The convolution of two finite length sequences can be
calculated by the following macro.
\verbatok{\MACROSSIGLIB/convol.sci}
\end{verbatim}
The syntax and use of this macro are explained in the following section.

\subsection{Use of the {\tt convol} Macro}
\index{macro syntax!convol@{\tt convol}}
\index{convolution!macro syntax}

	The {\tt convol} macro can be used following two formats.
The first format calculates a convolution based on two discrete
length sequences which are passed in their entirety to the macro.
The second format performs updated convolutions using the overlap-add
method described in \cite{oppen}.  It is used when one of the sequences
is very long and, consequently, cannot be passed directly to the macro.

	The syntax used for the macro under the first format is
\begin{verbatim}
-->y=convol(h,x)
\end{verbatim}
where both {\tt h} and {\tt x} are finite length vectors and {\tt y}
is a vector representing the resulting convolution of the inputs.
An example of the use of the macro under the first format is as follows.
\verbatok{\Diary convol.2.dia}
\end{verbatim}

	The syntax used for the macro under the second format is
\begin{verbatim}
-->[y,y1]=convol(h,x,y0)
\end{verbatim}
where {\tt y0} and {\tt y1} are required to update the calculations
of the convolution at each iteration and 
where the use of the second format requires the following macro
supplied by the user.
\verbatok{\Diary convol.1.code}
\end{verbatim}
where, {\tt nsecs} is the number of sections of $x$ to be used
in the convolution calculation and, in addition, 
the user must supply a macro {\tt getx} which
obtains segments of the data $x$ following the format.
\begin{verbatim}
//[xv]=getx(xlen,xstart)
   .
   .
   .
//end
\end{verbatim}
where {\tt xlen} is the length of data requested and {\tt xstart}
is the length of the data vector to be used.

\section{The Chirp Z-Transform}
\index{chirp z-transform}

\subsection{Introduction}

	The discrete Fourier transform (DFT) of a finite length,
discrete time signal, $x(n)$, is defined by
%
\begin{eqnarray}
X(k) &=& \sum_{n=0}^{N-1}x(n)e^{-j(2\pi nk)/N}\\
n&=&0,1,\ldots,N-1\nonumber\\
k&=&0,1,\ldots,N-1\nonumber
\label{e4.1}
\end{eqnarray}
%
and the z-transform of $x(n)$ is given by
%
\begin{eqnarray}
X(z) &=& \sum_{n=-\infty}^{\infty}x(n)z^{-n}\nonumber\\
     &=& \sum_{n=0}^{N-1}x(n)z^{-n}
\label{e4.2}
\end{eqnarray}
%
The $N-1$ points of the DFT
of $x(n)$ are related to the z-transform of $x(n)$ in that
they are samples of the z-transform taken at equally spaced
intervals on the unit circle in the z-plane.
The fast Fourier transform (FFT) algorithm is an algorithm
which calculates (\ref{e4.1}) very efficiently.

	There are applications \cite{czt} where it is desired to calculate
samples of the z-transform at locations either off the unit circle or at
unequally spaced angles on the unit circle.  The chirp
z-transform (CZT) is an efficient algorithm which can be used
for calculating samples of some of these z-transforms.  In particular,
the CZT can be used to efficiently calculate the values of the
z-transform of a finite-length, discrete-time sequence if the
z-transform points are of the form
%
\begin{equation}
z_k = AW^{-k}
\label{e4.3a}
\end{equation}
%
where
%
\begin{eqnarray}
A   &=& A_0e^{j\theta}\nonumber\\
W   &=& W_0e^{-j\phi}
\label{e4.3}
\end{eqnarray}
%
and where $A_0$ and $W_0$ are real valued constants and $\theta$ and $\phi$
are angles.
%
\input{\Figdir czt.1.tex}
\dessin{{\tt exec('czt.1.code')} Samples of the z-transform on Spirals}{f4.1}
e set of points $\{z_k\}$ lie on a spiral where
$z_0$ is at distance $A_0$ from the origin and at angle
$\theta$ from the x-axis.  The remaining points are located
at equally spaced angles, $\phi$, and approach the origin for
$W_0>1$, move away from the origin for $W_0<1$, and remain
on a circle of radius $A_0$ for $W_0=1$.  Figure~\ref{f4.1}
shows the location of samples of the z-transform for $W_0< 1$
on the left hand side of the figure and of
$W_0<1$ on the right hand side of the figure. In both parts of the
figure the position of 
$z_0$ is indicated by the sample connected to the origin by a straight
line.
\subsection{Calculating the CZT}

	Calculating samples of the z-transform
of $x(n)$ at the $M$ points designated in (\ref{e4.3})
requires that
%
\begin{equation}
\begin{array}{cc}
{\displaystyle X(z_k)=\sum_{n=0}^{N-1}x(n)A^{-n}W^{nk}}, & k=0,1,\ldots,M-1
\end{array}
\label{e4.4}
\end{equation}
%
where $N$ is the length of $x(n)$.  Using the identity
%
\begin{equation}
nk=\frac{1}{2}[n^2+k^2-(k-n)^2]
\label{e4.5}
\end{equation}
%
in (\ref{e4.4}) yields
%
\begin{eqnarray}
X(z_k)&=&\sum_{n=0}^{N-1}x(n)A^{-n}W^{\frac{1}{2}n^2}W^{\frac{1}{2}k^2}
W^{-\frac{1}{2}(k-n)^2}\nonumber\\
      &=& W^{\frac{1}{2}k^2}\sum_{n=0}^{N-1}[x(n)A^{-n}W^{\frac{1}{2}n^2}
W^{-\frac{1}{2}(k-n)^2}].
\label{e4.6}
\end{eqnarray}
%
It can be seen that imbedded in (\ref{e4.6}) is a convolution of two
sequences $g(n)$ and $h(n)$ where
%
\begin{equation}
g(n)=x(n)A^{-n}W^{\frac{1}{2}n^2}
\label{e4.7}
\end{equation}
%
and
%
\begin{equation}
h(n)=W^{-\frac{1}{2}n^2}.
\label{e4.8}
\end{equation}
%
Consequently, (\ref{e4.6}) can be represented by the block
diagram in Figure~\ref{f4.2}.  (The circular junctions in
Figure~\ref{f4.2} represent multiplication of the two incoming
signals).
%
\begin{figure}[tb]
\center{
\begin{picture}(360,204)

\put(95,120){\makebox(0,0){$x(n)$}}
\put(110,120){\vector(1,0){10}}
\put(125,120){\circle{10}}
\put(130,120){\vector(1,0){15}}

\put(145,100){\framebox(65,40){$h(n)$}}
\put(210,120){\vector(1,0){15}}

\put(230,120){\circle{10}}
\put(235,120){\vector(1,0){10}}
\put(260,120){\makebox(0,0){$X(k)$}}

\put(230,85){\vector(0,1){30}}
\put(230,75){\makebox(0,0){$1/h(k)$}}
\put(125,85){\vector(0,1){30}}
\put(125,75){\makebox(0,0){$A^{-n}/h(n)$}}

\end{picture}
}
\caption{Filter Realization of CZT}
\label{f4.2}
\end{figure}
%

	The convolution in (\ref{e4.6}) can be efficiently
implemented using an FFT.  Since the input sequence
is of length $N$ and the output sequence is of length $M$, it is 
necessary to use $N+M-1$ elements from $h(n)$.
These $N+M-1$ elements are $h(-N+1), h(-N+2),\ldots,h(n),\ldots,
h(M-2),h(M-1)$.  After taking the product of the $N+M-1$ point
FFT of $h(n)$ and of $g(n)$
(where $M-1$ zero points have been added to the end of $g(n)$),
the inverse FFT yields a circular 
convolution of $h(n)$ and $g(n)$.  Care must be taken to
choose the correct $M$ points corresponding to the linear
convolution desired.
\subsection{How to Use the Macro {\tt czt}}
\index{macro syntax!czt@{\tt czt}}
\index{chirp z-transform!macro syntax}

	The macro {\tt czt} implements the
chirp z-transform.	
the syntax for {\tt czt} is as follows
{\tt
\begin{verbatim}
--> [czx]=czt(x,m,w,a,phi,theta)
\end{verbatim}}
where {\tt x} is an input vector of unspecified
length $N$.  The output sequence {\tt czx}, is the 
CZT of {\tt x} taken at the {\tt m} points in the z-plane
%
\begin{equation}
z_k=\mtt{ a}e^{j\cdot\mtt{ theta}}\mtt{ w}^ke^{jk\cdot \mtt{ phi}}.
\end{equation}
%
for $k=0,1,\ldots,\mtt{ m}-1$.

\subsection{Examples}
\index{chirp z-transform!examples}

	The first example presented here calculates the 
CZT of the sequence $x(n)=n$ for $n=0,1,\ldots,9$ where
ten points are calculated in the z-plane and the parameter values
are $W=1$, $\phi=2\pi/10$, $A=1$, and $\theta=0$.  This
example should yield results identical to those obtained by
taking the FFT of the sequence $x(n)$.  The sequence of commands is 
as follows,
\verbatok{\Diary czt.2.dia}
\end{verbatim}
As can be verified using the macro {\tt fft}, the above result is
identical to that obtained by taking the FFT of the sequence $x(n)$
which is shown below,
\verbatok{\Diary czt.3.dia}
\end{verbatim}

	The second example calculates the DFT of the same sequence,
$x(n)$, above, however, just at five equally spaced points in $[-\pi/4,\pi/4]$
on the unit circle in the z-plane.  The spacing between the points is
$\pi/8$ for five points in $[-\pi/4,\pi/4]$.  The result is
\verbatok{\Diary czt.4.dia}
\end{verbatim}
Now taking a sixteen point FFT of the sequence $x(n)$ (accomplished
by adding six zeros to the end of the sequence $x(n)$) it can be
seen that the CZT computed above yields exactly the desired points
on the unit circle in the z-plane.  That is to say that the last
three points of {\tt czx} correspond to the first three points
of the FFT of $x(n)$ and the first two points of {\tt czx} correspond
to the last two points of the FFT.
\verbatok{\Diary czt.5.dia}
\end{verbatim}

\subsection{Macro Code}
\index{macro code!czt@{\tt czt}}
\index{chirp z-transform!macro code}

\verbatok{\MACROSSIGLIB/czt.sci}
\end{verbatim}

 
%\end{document}